{"cells":[{"cell_type":"code","source":["# Credit Card Fraud Detection   https://www.kaggle.com/mlg-ulb/creditcardfraud\n\n# Goal of this model to detect the fradulent CC transacations\n\n# LogisticRegression & Random Forest "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Read the csv file and cache the data\nfrom pyspark.sql import SQLContext\n\nfrom pyspark import SparkContext\n\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.read.csv('/FileStore/tables/creditcard.csv',header='true', inferSchema='true')\ndf.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>DataFrame[Time: decimal(10,0), V1: double, V2: double, V3: double, V4: double, V5: double, V6: double, V7: double, V8: double, V9: double, V10: double, V11: double, V12: double, V13: double, V14: double, V15: double, V16: double, V17: double, V18: double, V19: double, V20: double, V21: double, V22: double, V23: double, V24: double, V25: double, V26: double, V27: double, V28: double, Amount: double, Class: int]\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Look at the count of distinct records: Normal vs Fraudelent transactions;\ndf.select(\"Class\").distinct().show()\ndf.groupBy(\"Class\").count().show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+\nClass|\n+-----+\n    1|\n    0|\n+-----+\n\n+-----+------+\nClass| count|\n+-----+------+\n    1|   492|\n    0|284315|\n+-----+------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Drop the Time feature as we are not relying on it\n# Rename the class to label for BinaryClassificationEvaluator\n\ndf = df.drop(\"Time\")\ndf = df.withColumnRenamed(\"Class\", \"label\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["from  pyspark.ml.feature import StandardScaler\nfrom  pyspark.ml.feature import VectorAssembler\n\n# Vectorize the amount column\namt_assembler = VectorAssembler(inputCols = ['Amount'],outputCol = \"vec_Amount\")\n\ndf = amt_assembler.transform(df)\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Now Standardize the Amount column, iternal Stats function \nstandardizer = StandardScaler(withMean=True, withStd=True,inputCol='vec_Amount',outputCol='std_Amount')\nmodel = standardizer.fit(df)\ndf = model.transform(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# vectorize all the features columns\nsel_cols = ['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','std_Amount']\nassembler = VectorAssembler(inputCols = sel_cols,outputCol = \"features\")\ndf = assembler.transform(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Split the dataset into training/testing for Random Classifier, seed is justa random sample data fucntion \n(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\nfrom pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\nrfModel = rf.fit(trainingData)\npredictions = rfModel.transform(testData)\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"Amount\")\nselected.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+--------------------+-------+\nlabel|prediction|         probability| Amount|\n+-----+----------+--------------------+-------+\n    0|       0.0|[0.99775801475801...| 2520.0|\n    1|       1.0|[0.29922294067656...|  99.99|\n    0|       0.0|[0.99572544235796...|4313.82|\n    0|       1.0|[0.29922294067656...|    1.0|\n    1|       1.0|[0.29922294067656...|  99.99|\n    1|       1.0|[0.29922294067656...|  99.99|\n    0|       0.0|[0.94298975661087...| 845.73|\n    0|       0.0|[0.98809389399204...|    1.0|\n    0|       0.0|[0.98797012663074...|    1.0|\n    1|       1.0|[0.29922294067656...|  99.99|\n    1|       1.0|[0.29922294067656...|  99.99|\n    0|       0.0|[0.98861145819002...|    1.0|\n    0|       0.0|[0.99337376589921...|   15.0|\n    1|       1.0|[0.07170173939863...|    1.0|\n    0|       0.0|[0.99738699817502...| 325.56|\n    1|       1.0|[0.05975076902069...|    1.0|\n    1|       1.0|[0.05975076902069...|    1.0|\n    0|       0.0|[0.99914839841505...|  99.99|\n    0|       0.0|[0.99914839841505...|  99.99|\n    1|       1.0|[0.07170173939863...|    1.0|\n+-----+----------+--------------------+-------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Now evaluate the model \nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>0.8086795456779414\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>0.979717113387102\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Now build the Logistic Regression \nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\nlrModel = lr.fit(trainingData)\npredictions = lrModel.transform(testData)\nevaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">10</span><span class=\"ansired\">]: </span>0.9768333963546695\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n# Area Under PR is less for logistic Regression; hence RandomForest is the best model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">11</span><span class=\"ansired\">]: </span>0.763135898517088\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Cross validate the RandomForest model to get best model\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid,\nevaluator=evaluator, numFolds=10)\ncvModel = cv.fit(trainingData)\npredictions = cvModel.transform(testData)\nevaluator.evaluate(predictions)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Area under PR is slighly improved, let's pick the bestModel\nevaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>0.8080410567362993\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["bestModel = cvModel.bestModel\nfinalPredictions = bestModel.transform(df)\nevaluator.evaluate(finalPredictions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>0.9800805859297558\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["evaluator.evaluate(finalPredictions, {evaluator.metricName: \"areaUnderPR\"})\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">17</span><span class=\"ansired\">]: </span>0.8075707132031913\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"CCFraud Detection","notebookId":4266191371845891},"nbformat":4,"nbformat_minor":0}
